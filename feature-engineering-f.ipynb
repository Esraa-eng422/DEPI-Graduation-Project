{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-11-29T18:15:28.139669Z",
     "iopub.status.busy": "2025-11-29T18:15:28.139263Z",
     "iopub.status.idle": "2025-11-29T18:15:30.950807Z",
     "shell.execute_reply": "2025-11-29T18:15:30.949461Z",
     "shell.execute_reply.started": "2025-11-29T18:15:28.139635Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/m5-forecasting-accuracy/calendar.csv\n",
      "/kaggle/input/m5-forecasting-accuracy/sample_submission.csv\n",
      "/kaggle/input/m5-forecasting-accuracy/sell_prices.csv\n",
      "/kaggle/input/m5-forecasting-accuracy/sales_train_validation.csv\n",
      "/kaggle/input/m5-forecasting-accuracy/sales_train_evaluation.csv\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-29T18:15:30.953726Z",
     "iopub.status.busy": "2025-11-29T18:15:30.953205Z",
     "iopub.status.idle": "2025-11-29T18:15:30.962856Z",
     "shell.execute_reply": "2025-11-29T18:15:30.961633Z",
     "shell.execute_reply.started": "2025-11-29T18:15:30.953687Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import warnings\n",
    "from math import ceil\n",
    "import gc\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Configuration**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-29T18:15:30.964208Z",
     "iopub.status.busy": "2025-11-29T18:15:30.963902Z",
     "iopub.status.idle": "2025-11-29T18:15:31.051265Z",
     "shell.execute_reply": "2025-11-29T18:15:31.049956Z",
     "shell.execute_reply.started": "2025-11-29T18:15:30.964182Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class Config:\n",
    "    \"\"\"Main configuration for feature engineering\"\"\"\n",
    "    \n",
    "    # Data paths\n",
    "    DATA_PATH = '../input/m5-forecasting-accuracy/'\n",
    "    TRAIN_FILE = 'sales_train_validation.csv'\n",
    "    PRICES_FILE = 'sell_prices.csv'\n",
    "    CALENDAR_FILE = 'calendar.csv'\n",
    "    \n",
    "    # Output file\n",
    "    OUTPUT_FILE = 'processed_data.pkl'\n",
    "    \n",
    "    # Parameters\n",
    "    TARGET = 'sales'\n",
    "    END_TRAIN = 1913\n",
    "    VALIDATION_DAYS = 28\n",
    "    START_DAY = 1000  # Use recent data to save memory\n",
    "    \n",
    "    # Feature engineering settings\n",
    "    LAGS = [7, 28]  # Weekly and monthly lags\n",
    "    ROLLING_WINDOWS = [7, 28]  # Rolling window sizes\n",
    "    \n",
    "    # Random state for reproducibility\n",
    "    RANDOM_STATE = 42\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **DATA LOADING**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-29T18:15:31.052962Z",
     "iopub.status.busy": "2025-11-29T18:15:31.052611Z",
     "iopub.status.idle": "2025-11-29T18:15:31.075661Z",
     "shell.execute_reply": "2025-11-29T18:15:31.074614Z",
     "shell.execute_reply.started": "2025-11-29T18:15:31.052929Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    \"\"\"Load all required datasets\"\"\"\n",
    "    print('='*80)\n",
    "    print('STEP 1: LOADING DATA')\n",
    "    print('='*80)\n",
    "    \n",
    "    print('\\nLoading files...')\n",
    "    train = pd.read_csv(os.path.join(Config.DATA_PATH, Config.TRAIN_FILE))\n",
    "    prices = pd.read_csv(os.path.join(Config.DATA_PATH, Config.PRICES_FILE))\n",
    "    calendar = pd.read_csv(os.path.join(Config.DATA_PATH, Config.CALENDAR_FILE))\n",
    "    \n",
    "    print(f' Train dataset: {train.shape}')\n",
    "    print(f' Prices dataset: {prices.shape}')\n",
    "    print(f' Calendar dataset: {calendar.shape}')\n",
    "    \n",
    "    return train, prices, calendar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **CREATE BASE GRID**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-29T18:15:31.077420Z",
     "iopub.status.busy": "2025-11-29T18:15:31.076929Z",
     "iopub.status.idle": "2025-11-29T18:15:31.108947Z",
     "shell.execute_reply": "2025-11-29T18:15:31.107410Z",
     "shell.execute_reply.started": "2025-11-29T18:15:31.077385Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def create_base_grid(train_df):\n",
    "    \"\"\"\n",
    "    Transform wide format to long format\n",
    "    Convert from horizontal (item x days) to vertical (item x day)\n",
    "    \"\"\"\n",
    "    print('\\n' + '='*80)\n",
    "    print('STEP 2: CREATING BASE GRID')\n",
    "    print('='*80)\n",
    "    \n",
    "    # Define index columns (item identifiers)\n",
    "    index_cols = ['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id']\n",
    "    \n",
    "    print('\\nTransforming data from wide to long format...')\n",
    "    # Melt: convert columns (d_1, d_2, ...) to rows\n",
    "    grid = pd.melt(\n",
    "        train_df,\n",
    "        id_vars=index_cols,\n",
    "        var_name='d',\n",
    "        value_name=Config.TARGET\n",
    "    )\n",
    "    \n",
    "    print(f' Grid created with {len(grid):,} rows')\n",
    "    \n",
    "    # Add test period (future 28 days)\n",
    "    print('\\nAdding test period rows...')\n",
    "    test_rows = []\n",
    "    for i in range(1, 29):\n",
    "        temp = train_df[index_cols].copy()\n",
    "        temp['d'] = f'd_{Config.END_TRAIN + i}'\n",
    "        temp[Config.TARGET] = np.nan\n",
    "        test_rows.append(temp)\n",
    "    \n",
    "    test_grid = pd.concat(test_rows, ignore_index=True)\n",
    "    grid = pd.concat([grid, test_grid], ignore_index=True)\n",
    "    \n",
    "    print(f' Added test period: {len(test_grid):,} rows')\n",
    "    print(f' Total grid size: {len(grid):,} rows')\n",
    "    \n",
    "    # Convert 'd' column to numeric (d_1 -> 1)\n",
    "    print('\\nOptimizing data types...')\n",
    "    grid['d'] = grid['d'].str.replace('d_', '').astype(np.int16)\n",
    "    \n",
    "    # Convert categorical columns to save memory\n",
    "    for col in index_cols:\n",
    "        grid[col] = grid[col].astype('category')\n",
    "    \n",
    "    print(f' Memory optimized')\n",
    "    \n",
    "    return grid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **ADD PRICE FEATURES**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-29T18:15:31.111510Z",
     "iopub.status.busy": "2025-11-29T18:15:31.110221Z",
     "iopub.status.idle": "2025-11-29T18:15:31.140061Z",
     "shell.execute_reply": "2025-11-29T18:15:31.138801Z",
     "shell.execute_reply.started": "2025-11-29T18:15:31.111474Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def add_price_features(grid, prices, calendar):\n",
    "    \"\"\"Add price-related features to the grid\"\"\"\n",
    "    print('\\n' + '='*80)\n",
    "    print('STEP 3: ADDING PRICE FEATURES')\n",
    "    print('='*80)\n",
    "    \n",
    "    # Get week information from calendar\n",
    "    print('\\nMerging calendar week information...')\n",
    "    calendar_prices = calendar[['wm_yr_wk', 'd']].drop_duplicates()\n",
    "    calendar_prices['d'] = calendar_prices['d'].str.replace('d_', '').astype(np.int16)\n",
    "    \n",
    "    grid = grid.merge(calendar_prices, on='d', how='left')\n",
    "    \n",
    "    # Calculate price statistics per item\n",
    "    print('Calculating price statistics...')\n",
    "    price_stats = prices.groupby(['store_id', 'item_id'])['sell_price'].agg([\n",
    "        ('price_max', 'max'),\n",
    "        ('price_min', 'min'),\n",
    "        ('price_mean', 'mean'),\n",
    "        ('price_std', 'std')\n",
    "    ]).reset_index()\n",
    "    \n",
    "    # Merge statistics to prices\n",
    "    prices = prices.merge(price_stats, on=['store_id', 'item_id'], how='left')\n",
    "    \n",
    "    # Create normalized price (0-1 range)\n",
    "    prices['price_norm'] = prices['sell_price'] / prices['price_max']\n",
    "    \n",
    "    # Merge prices to grid\n",
    "    print('Merging prices to grid...')\n",
    "    grid = grid.merge(\n",
    "        prices[['store_id', 'item_id', 'wm_yr_wk', 'sell_price', \n",
    "                'price_norm', 'price_max', 'price_min', 'price_mean', 'price_std']],\n",
    "        on=['store_id', 'item_id', 'wm_yr_wk'],\n",
    "        how='left'\n",
    "    )\n",
    "    \n",
    "    # Drop temporary week column\n",
    "    grid.drop(columns=['wm_yr_wk'], inplace=True)\n",
    "    \n",
    "    print(f' Price features added')\n",
    "    print(f' Grid shape: {grid.shape}')\n",
    "    \n",
    "    return grid\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **ADD CALENDAR FEATURES**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-29T18:15:31.143215Z",
     "iopub.status.busy": "2025-11-29T18:15:31.142870Z",
     "iopub.status.idle": "2025-11-29T18:15:31.170186Z",
     "shell.execute_reply": "2025-11-29T18:15:31.169110Z",
     "shell.execute_reply.started": "2025-11-29T18:15:31.143181Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def add_calendar_features(grid, calendar):\n",
    "    \"\"\"Add time-based and event features\"\"\"\n",
    "    print('\\n' + '='*80)\n",
    "    print('STEP 4: ADDING CALENDAR FEATURES')\n",
    "    print('='*80)\n",
    "    \n",
    "    # Prepare calendar data\n",
    "    calendar_temp = calendar.copy()\n",
    "    calendar_temp['d'] = calendar_temp['d'].str.replace('d_', '').astype(np.int16)\n",
    "    \n",
    "    # Select relevant calendar columns\n",
    "    cal_cols = [\n",
    "        'd', 'date', \n",
    "        'event_name_1', 'event_type_1',\n",
    "        'event_name_2', 'event_type_2',\n",
    "        'snap_CA', 'snap_TX', 'snap_WI'\n",
    "    ]\n",
    "    \n",
    "    print('\\nMerging calendar information...')\n",
    "    grid = grid.merge(calendar_temp[cal_cols], on='d', how='left')\n",
    "    \n",
    "    # Extract date features\n",
    "    print('Extracting date features...')\n",
    "    grid['date'] = pd.to_datetime(grid['date'])\n",
    "    \n",
    "    # Basic date components\n",
    "    grid['day'] = grid['date'].dt.day.astype(np.int8)\n",
    "    grid['week'] = grid['date'].dt.isocalendar().week.astype(np.int8)\n",
    "    grid['month'] = grid['date'].dt.month.astype(np.int8)\n",
    "    grid['year'] = grid['date'].dt.year.astype(np.int16)\n",
    "    grid['dayofweek'] = grid['date'].dt.dayofweek.astype(np.int8)\n",
    "    \n",
    "    # Derived features\n",
    "    grid['is_weekend'] = (grid['dayofweek'] >= 5).astype(np.int8)\n",
    "    grid['is_month_start'] = (grid['day'] <= 7).astype(np.int8)\n",
    "    grid['is_month_end'] = (grid['day'] >= 24).astype(np.int8)\n",
    "    \n",
    "    # Drop date column (already extracted features)\n",
    "    grid.drop(columns=['date'], inplace=True)\n",
    "    \n",
    "    # Convert categorical columns\n",
    "    cat_cols = ['event_name_1', 'event_type_1', 'event_name_2', 'event_type_2']\n",
    "    for col in cat_cols:\n",
    "        grid[col] = grid[col].astype('category')\n",
    "    \n",
    "    print(f' Calendar features added')\n",
    "    print(f' Grid shape: {grid.shape}')\n",
    "    \n",
    "    return grid\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **ADD LAG FEATURES**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-29T18:15:31.171631Z",
     "iopub.status.busy": "2025-11-29T18:15:31.171208Z",
     "iopub.status.idle": "2025-11-29T18:15:31.201707Z",
     "shell.execute_reply": "2025-11-29T18:15:31.200376Z",
     "shell.execute_reply.started": "2025-11-29T18:15:31.171500Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def add_lag_features(grid):\n",
    "    \"\"\"\n",
    "    Add lag features - previous sales values\n",
    "    These are crucial for time series forecasting\n",
    "    \"\"\"\n",
    "    print('\\n' + '='*80)\n",
    "    print('STEP 5: ADDING LAG FEATURES')\n",
    "    print('='*80)\n",
    "    \n",
    "    # Filter to recent data to save memory\n",
    "    print(f'\\nFiltering data from day {Config.START_DAY}...')\n",
    "    grid = grid[grid['d'] >= Config.START_DAY].reset_index(drop=True)\n",
    "    print(f' Grid size after filtering: {len(grid):,} rows')\n",
    "    \n",
    "    # Create lag features\n",
    "    print('\\nCreating lag features:')\n",
    "    for lag in Config.LAGS:\n",
    "        print(f'  - lag_{lag} (sales from {lag} days ago)')\n",
    "        grid[f'lag_{lag}'] = grid.groupby('id')[Config.TARGET].transform(\n",
    "            lambda x: x.shift(lag)\n",
    "        )\n",
    "    \n",
    "    print(f' Lag features added')\n",
    "    print(f' Grid shape: {grid.shape}')\n",
    "    \n",
    "    return grid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **ADD ROLLING FEATURES**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-29T18:15:31.203234Z",
     "iopub.status.busy": "2025-11-29T18:15:31.202890Z",
     "iopub.status.idle": "2025-11-29T18:15:31.234628Z",
     "shell.execute_reply": "2025-11-29T18:15:31.233332Z",
     "shell.execute_reply.started": "2025-11-29T18:15:31.203203Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def add_rolling_features(grid):\n",
    "    \"\"\"\n",
    "    Add rolling window statistics\n",
    "    These capture trends and patterns over time\n",
    "    \"\"\"\n",
    "    print('\\n' + '='*80)\n",
    "    print('STEP 6: ADDING ROLLING WINDOW FEATURES')\n",
    "    print('='*80)\n",
    "    \n",
    "    print('\\nCreating rolling features:')\n",
    "    for window in Config.ROLLING_WINDOWS:\n",
    "        print(f'\\n  Window size: {window} days')\n",
    "        \n",
    "        # Shift by 28 days to prevent data leakage\n",
    "        shifted = grid.groupby('id')[Config.TARGET].transform(\n",
    "            lambda x: x.shift(28)\n",
    "        )\n",
    "        \n",
    "        # Rolling statistics\n",
    "        print(f'    - rolling_mean_{window}')\n",
    "        grid[f'rolling_mean_{window}'] = shifted.rolling(window).mean()\n",
    "        \n",
    "        print(f'    - rolling_std_{window}')\n",
    "        grid[f'rolling_std_{window}'] = shifted.rolling(window).std()\n",
    "        \n",
    "        print(f'    - rolling_min_{window}')\n",
    "        grid[f'rolling_min_{window}'] = shifted.rolling(window).min()\n",
    "        \n",
    "        print(f'    - rolling_max_{window}')\n",
    "        grid[f'rolling_max_{window}'] = shifted.rolling(window).max()\n",
    "    \n",
    "    print(f'\\n Rolling features added')\n",
    "    print(f' Grid shape: {grid.shape}')\n",
    "    \n",
    "    return grid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **ADD CUSTOM FEATURES**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-29T18:15:31.236355Z",
     "iopub.status.busy": "2025-11-29T18:15:31.236001Z",
     "iopub.status.idle": "2025-11-29T18:15:31.261654Z",
     "shell.execute_reply": "2025-11-29T18:15:31.260425Z",
     "shell.execute_reply.started": "2025-11-29T18:15:31.236326Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def add_custom_features(grid):\n",
    "    \"\"\"\n",
    "    Add custom engineered features\n",
    "    These are domain-specific features that may improve prediction\n",
    "    \"\"\"\n",
    "    print('\\n' + '='*80)\n",
    "    print('STEP 7: ADDING CUSTOM FEATURES')\n",
    "    print('='*80)\n",
    "    \n",
    "    print('\\nCreating custom features:')\n",
    "    \n",
    "    # 1. Holiday indicators\n",
    "    print('  - Holiday indicators (christmas, thanksgiving, new_year)')\n",
    "    grid['is_christmas_season'] = (\n",
    "        (grid['month'] == 12) & (grid['day'] >= 15)\n",
    "    ).astype(np.int8)\n",
    "    \n",
    "    grid['is_thanksgiving'] = (\n",
    "        (grid['month'] == 11) & (grid['week'] == 4)\n",
    "    ).astype(np.int8)\n",
    "    \n",
    "    grid['is_new_year'] = (\n",
    "        (grid['month'] == 1) & (grid['day'] <= 7)\n",
    "    ).astype(np.int8)\n",
    "    \n",
    "    # 2. Sales velocity (rate of change)\n",
    "    if 'lag_7' in grid.columns and 'lag_28' in grid.columns:\n",
    "        print('  - Sales velocity')\n",
    "        grid['sales_velocity'] = (grid['lag_7'] - grid['lag_28']) / 21.0\n",
    "    \n",
    "    # 3. Price change indicators\n",
    "    if 'sell_price' in grid.columns:\n",
    "        print('  - Price change indicators')\n",
    "        grid['price_change'] = grid.groupby(['store_id', 'item_id'])['sell_price'].diff()\n",
    "        grid['price_increased'] = (grid['price_change'] > 0).astype(np.int8)\n",
    "    \n",
    "    # 4. Seasonal indicators\n",
    "    print('  - Seasonal indicators (summer, winter)')\n",
    "    grid['is_summer'] = grid['month'].isin([6, 7, 8]).astype(np.int8)\n",
    "    grid['is_winter'] = grid['month'].isin([12, 1, 2]).astype(np.int8)\n",
    "    \n",
    "    # 5. Week of month\n",
    "    print('  - Week of month')\n",
    "    grid['week_of_month'] = ((grid['day'] - 1) // 7 + 1).astype(np.int8)\n",
    "    \n",
    "    print(f'\\n Custom features added')\n",
    "    print(f' Grid shape: {grid.shape}')\n",
    "    \n",
    "    return grid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **HANDLE MISSING VALUES**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-29T18:15:31.263108Z",
     "iopub.status.busy": "2025-11-29T18:15:31.262799Z",
     "iopub.status.idle": "2025-11-29T18:15:31.292809Z",
     "shell.execute_reply": "2025-11-29T18:15:31.291814Z",
     "shell.execute_reply.started": "2025-11-29T18:15:31.263083Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def handle_missing_values(grid):\n",
    "    \"\"\"\n",
    "    Intelligently handle missing values\n",
    "    Different strategies for different feature types\n",
    "    \"\"\"\n",
    "    print('\\n' + '='*80)\n",
    "    print('STEP 8: HANDLING MISSING VALUES')\n",
    "    print('='*80)\n",
    "    \n",
    "    print(f'\\nMissing values before: {grid.isnull().sum().sum():,}')\n",
    "    \n",
    "    # 1. Lag features - forward fill (use last known value)\n",
    "    print('\\nHandling lag features:')\n",
    "    lag_cols = [col for col in grid.columns if 'lag_' in col]\n",
    "    if lag_cols:\n",
    "        print(f'  - Forward filling {len(lag_cols)} lag features')\n",
    "        for col in lag_cols:\n",
    "            grid[col] = grid.groupby('id')[col].fillna(method='ffill').fillna(0)\n",
    "    \n",
    "    # 2. Rolling features - use median\n",
    "    print('\\nHandling rolling features:')\n",
    "    rolling_cols = [col for col in grid.columns if 'rolling_' in col]\n",
    "    if rolling_cols:\n",
    "        print(f'  - Median filling {len(rolling_cols)} rolling features')\n",
    "        for col in rolling_cols:\n",
    "            grid[col] = grid.groupby('id')[col].transform(\n",
    "                lambda x: x.fillna(x.median())\n",
    "            ).fillna(0)\n",
    "    \n",
    "    # 3. Price features - backward then forward fill\n",
    "    print('\\nHandling price features:')\n",
    "    price_cols = ['sell_price', 'price_norm', 'price_max', 'price_min', \n",
    "                  'price_mean', 'price_std']\n",
    "    for col in price_cols:\n",
    "        if col in grid.columns:\n",
    "            print(f'  - Filling {col}')\n",
    "            grid[col] = grid.groupby(['store_id', 'item_id'])[col].fillna(method='bfill')\n",
    "            grid[col] = grid.groupby(['store_id', 'item_id'])[col].fillna(method='ffill')\n",
    "    \n",
    "    # 4. Fill remaining NaN - HANDLE CATEGORICAL SEPARATELY!\n",
    "    print('\\nFilling remaining NaN...')\n",
    "    \n",
    "    # Get categorical columns\n",
    "    categorical_cols = grid.select_dtypes(include=['category']).columns.tolist()\n",
    "    numerical_cols = [col for col in grid.columns if col not in categorical_cols]\n",
    "    \n",
    "    # Fill numerical columns with 0\n",
    "    print(f'  - Filling {len(numerical_cols)} numerical columns with 0')\n",
    "    for col in numerical_cols:\n",
    "        if grid[col].isnull().any():\n",
    "            grid[col] = grid[col].fillna(0)\n",
    "    \n",
    "    # Fill categorical columns with 'Unknown' or most frequent\n",
    "    print(f'  - Filling {len(categorical_cols)} categorical columns')\n",
    "    for col in categorical_cols:\n",
    "        if grid[col].isnull().any():\n",
    "            # Add 'Unknown' to categories first\n",
    "            if 'Unknown' not in grid[col].cat.categories:\n",
    "                grid[col] = grid[col].cat.add_categories(['Unknown'])\n",
    "            grid[col] = grid[col].fillna('Unknown')\n",
    "    \n",
    "    print(f'\\n Missing values after: {grid.isnull().sum().sum():,}')\n",
    "    print(' All missing values handled')\n",
    "    \n",
    "    return grid\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **FEATURE SELECTION**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-29T18:15:31.294181Z",
     "iopub.status.busy": "2025-11-29T18:15:31.293890Z",
     "iopub.status.idle": "2025-11-29T18:15:31.325120Z",
     "shell.execute_reply": "2025-11-29T18:15:31.324022Z",
     "shell.execute_reply.started": "2025-11-29T18:15:31.294158Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def get_feature_list(grid):\n",
    "    \"\"\"\n",
    "    Define which features to use for modeling\n",
    "    Separate features from IDs and target\n",
    "    \"\"\"\n",
    "    print('\\n' + '='*80)\n",
    "    print('STEP 9: FEATURE SELECTION')\n",
    "    print('='*80)\n",
    "    \n",
    "    # Columns to exclude from features (IDs and target)\n",
    "    exclude = [\n",
    "        'id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id',\n",
    "        'd', Config.TARGET\n",
    "    ]\n",
    "    \n",
    "    # Get all feature columns\n",
    "    features = [col for col in grid.columns if col not in exclude]\n",
    "    \n",
    "    # Identify categorical features\n",
    "    categorical = [\n",
    "        col for col in features \n",
    "        if grid[col].dtype.name == 'category' or grid[col].dtype == 'object'\n",
    "    ]\n",
    "    \n",
    "    print(f'\\n Total features: {len(features)}')\n",
    "    print(f'  - Categorical: {len(categorical)}')\n",
    "    print(f'  - Numerical: {len(features) - len(categorical)}')\n",
    "    \n",
    "    # Show sample features\n",
    "    print('\\nSample features (first 15):')\n",
    "    for i, feat in enumerate(features[:15], 1):\n",
    "        feat_type = 'categorical' if feat in categorical else 'numerical'\n",
    "        print(f'  {i:2d}. {feat:30s} ({feat_type})')\n",
    "    \n",
    "    if len(features) > 15:\n",
    "        print(f'  ... and {len(features) - 15} more features')\n",
    "    \n",
    "    return features, categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **TRAIN/VALIDATION/TEST SPLIT**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-29T18:15:31.326497Z",
     "iopub.status.busy": "2025-11-29T18:15:31.326206Z",
     "iopub.status.idle": "2025-11-29T18:15:31.348912Z",
     "shell.execute_reply": "2025-11-29T18:15:31.347441Z",
     "shell.execute_reply.started": "2025-11-29T18:15:31.326466Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def create_train_valid_split(grid):\n",
    "    \"\"\"\n",
    "    Split data into training, validation, and test sets\n",
    "    Important: Time-based split for time series!\n",
    "    \"\"\"\n",
    "    print('\\n' + '='*80)\n",
    "    print('STEP 10: TRAIN/VALIDATION/TEST SPLIT')\n",
    "    print('='*80)\n",
    "    \n",
    "    print('\\nCreating time-based splits...')\n",
    "    \n",
    "    # Training data: up to (END_TRAIN - VALIDATION_DAYS)\n",
    "    train_end = Config.END_TRAIN - Config.VALIDATION_DAYS\n",
    "    train_mask = grid['d'] <= train_end\n",
    "    train = grid[train_mask].copy()\n",
    "    \n",
    "    # Validation data: last VALIDATION_DAYS of training period\n",
    "    valid_mask = (grid['d'] > train_end) & (grid['d'] <= Config.END_TRAIN)\n",
    "    valid = grid[valid_mask].copy()\n",
    "    \n",
    "    # Test data: future predictions\n",
    "    test_mask = grid['d'] > Config.END_TRAIN\n",
    "    test = grid[test_mask].copy()\n",
    "    \n",
    "    # Remove rows with NaN target (only for train/valid)\n",
    "    train = train[train[Config.TARGET].notna()].reset_index(drop=True)\n",
    "    valid = valid[valid[Config.TARGET].notna()].reset_index(drop=True)\n",
    "    \n",
    "    print(f'\\n Training set: {len(train):,} rows')\n",
    "    print(f'    Date range: d_{train[\"d\"].min()} to d_{train[\"d\"].max()}')\n",
    "    \n",
    "    print(f'\\n Validation set: {len(valid):,} rows')\n",
    "    print(f'    Date range: d_{valid[\"d\"].min()} to d_{valid[\"d\"].max()}')\n",
    "    \n",
    "    print(f'\\n Test set: {len(test):,} rows')\n",
    "    print(f'    Date range: d_{test[\"d\"].min()} to d_{test[\"d\"].max()}')\n",
    "    \n",
    "    return train, valid, test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **SAVE PROCESSED DATA**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-29T18:15:31.350975Z",
     "iopub.status.busy": "2025-11-29T18:15:31.350702Z",
     "iopub.status.idle": "2025-11-29T18:15:31.382284Z",
     "shell.execute_reply": "2025-11-29T18:15:31.381191Z",
     "shell.execute_reply.started": "2025-11-29T18:15:31.350951Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def save_processed_data(grid, features, categorical):\n",
    "    \"\"\"Save the processed dataset and feature information\"\"\"\n",
    "    print('\\n' + '='*80)\n",
    "    print('STEP 11: SAVING PROCESSED DATA')\n",
    "    print('='*80)\n",
    "    \n",
    "    # Save main dataset\n",
    "    print(f'\\nSaving processed dataset to {Config.OUTPUT_FILE}...')\n",
    "    grid.to_pickle(Config.OUTPUT_FILE)\n",
    "    print(f' Dataset saved ({grid.shape[0]:,} rows, {grid.shape[1]:,} columns)')\n",
    "    \n",
    "    # Save feature information\n",
    "    print('\\nSaving feature information...')\n",
    "    feature_info = {\n",
    "        'features': features,\n",
    "        'categorical': categorical,\n",
    "        'all_columns': list(grid.columns),\n",
    "        'target': Config.TARGET\n",
    "    }\n",
    "    \n",
    "    with open('feature_info.pkl', 'wb') as f:\n",
    "        import pickle\n",
    "        pickle.dump(feature_info, f)\n",
    "    \n",
    "    print(' Feature information saved to feature_info.pkl')\n",
    "    \n",
    "    # Create feature summary\n",
    "    print('\\nCreating feature summary report...')\n",
    "    with open('feature_summary.txt', 'w') as f:\n",
    "        f.write('='*80 + '\\n')\n",
    "        f.write('M5 FORECASTING - FEATURE ENGINEERING SUMMARY\\n')\n",
    "        f.write('='*80 + '\\n\\n')\n",
    "        f.write(f'Total Features: {len(features)}\\n')\n",
    "        f.write(f'Categorical Features: {len(categorical)}\\n')\n",
    "        f.write(f'Numerical Features: {len(features) - len(categorical)}\\n\\n')\n",
    "        f.write('='*80 + '\\n')\n",
    "        f.write('FEATURE LIST\\n')\n",
    "        f.write('='*80 + '\\n\\n')\n",
    "        \n",
    "        # Group features by type\n",
    "        lag_feats = [f for f in features if 'lag_' in f]\n",
    "        rolling_feats = [f for f in features if 'rolling_' in f]\n",
    "        price_feats = [f for f in features if 'price' in f.lower()]\n",
    "        time_feats = [f for f in features if any(x in f for x in ['day', 'week', 'month', 'year'])]\n",
    "        event_feats = [f for f in features if 'event' in f]\n",
    "        custom_feats = [f for f in features if f not in lag_feats + rolling_feats + price_feats + time_feats + event_feats]\n",
    "        \n",
    "        f.write(f'Lag Features ({len(lag_feats)}):\\n')\n",
    "        for feat in lag_feats:\n",
    "            f.write(f'  - {feat}\\n')\n",
    "        \n",
    "        f.write(f'\\nRolling Features ({len(rolling_feats)}):\\n')\n",
    "        for feat in rolling_feats:\n",
    "            f.write(f'  - {feat}\\n')\n",
    "        \n",
    "        f.write(f'\\nPrice Features ({len(price_feats)}):\\n')\n",
    "        for feat in price_feats:\n",
    "            f.write(f'  - {feat}\\n')\n",
    "        \n",
    "        f.write(f'\\nTime Features ({len(time_feats)}):\\n')\n",
    "        for feat in time_feats:\n",
    "            f.write(f'  - {feat}\\n')\n",
    "        \n",
    "        f.write(f'\\nEvent Features ({len(event_feats)}):\\n')\n",
    "        for feat in event_feats:\n",
    "            f.write(f'  - {feat}\\n')\n",
    "        \n",
    "        f.write(f'\\nCustom Features ({len(custom_feats)}):\\n')\n",
    "        for feat in custom_feats:\n",
    "            f.write(f'  - {feat}\\n')\n",
    "    \n",
    "    print(' Feature summary saved to feature_summary.txt')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **MAIN PIPELINE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-29T18:15:31.384023Z",
     "iopub.status.busy": "2025-11-29T18:15:31.383670Z",
     "iopub.status.idle": "2025-11-29T18:15:31.411330Z",
     "shell.execute_reply": "2025-11-29T18:15:31.410314Z",
     "shell.execute_reply.started": "2025-11-29T18:15:31.383995Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# MAIN PIPELINE\n",
    "# ============================================================================\n",
    "\n",
    "def run_feature_engineering_pipeline():\n",
    "    \"\"\"Execute the complete feature engineering pipeline\"\"\"\n",
    "    \n",
    "    print('\\n' + '='*80)\n",
    "    print('M5 WALMART SALES FORECASTING')\n",
    "    print('PART 1: FEATURE ENGINEERING PIPELINE')\n",
    "    print('='*80 + '\\n')\n",
    "    \n",
    "    # Load data\n",
    "    train_df, prices_df, calendar_df = load_data()\n",
    "    \n",
    "    # Create base grid\n",
    "    grid = create_base_grid(train_df)\n",
    "    del train_df\n",
    "    gc.collect()\n",
    "    \n",
    "    # Add features\n",
    "    grid = add_price_features(grid, prices_df, calendar_df)\n",
    "    grid = add_calendar_features(grid, calendar_df)\n",
    "    del prices_df, calendar_df\n",
    "    gc.collect()\n",
    "    \n",
    "    grid = add_lag_features(grid)\n",
    "    grid = add_rolling_features(grid)\n",
    "    grid = add_custom_features(grid)\n",
    "    \n",
    "    # Handle missing values\n",
    "    grid = handle_missing_values(grid)\n",
    "    \n",
    "    # Feature selection\n",
    "    features, categorical = get_feature_list(grid)\n",
    "    \n",
    "    # Train/valid/test split\n",
    "    train, valid, test = create_train_valid_split(grid)\n",
    "    \n",
    "    # Save everything\n",
    "    save_processed_data(grid, features, categorical)\n",
    "    \n",
    "    # Final summary\n",
    "    print('\\n' + '='*80)\n",
    "    print('FEATURE ENGINEERING COMPLETED SUCCESSFULLY!')\n",
    "    print('='*80)\n",
    "    print('\\nGenerated files:')\n",
    "    print('   processed_data.pkl - Complete processed dataset')\n",
    "    print('   feature_info.pkl - Feature names and types')\n",
    "    print('   feature_summary.txt - Detailed feature report')\n",
    "    print('\\nDataset statistics:')\n",
    "    print(f'  - Total rows: {len(grid):,}')\n",
    "    print(f'  - Total columns: {len(grid.columns):,}')\n",
    "    print(f'  - Features: {len(features):,}')\n",
    "    print(f'  - Training samples: {len(train):,}')\n",
    "    print(f'  - Validation samples: {len(valid):,}')\n",
    "    print(f'  - Test samples: {len(test):,}')\n",
    "    print('\\nNext step: Run Part 2 (model_training.py) for model training!')\n",
    "    print('='*80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-29T18:15:31.412641Z",
     "iopub.status.busy": "2025-11-29T18:15:31.412345Z",
     "iopub.status.idle": "2025-11-29T18:28:36.470713Z",
     "shell.execute_reply": "2025-11-29T18:28:36.469008Z",
     "shell.execute_reply.started": "2025-11-29T18:15:31.412619Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "M5 WALMART SALES FORECASTING\n",
      "PART 1: FEATURE ENGINEERING PIPELINE\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "STEP 1: LOADING DATA\n",
      "================================================================================\n",
      "\n",
      "Loading files...\n",
      " Train dataset: (30490, 1919)\n",
      " Prices dataset: (6841121, 4)\n",
      " Calendar dataset: (1969, 14)\n",
      "\n",
      "================================================================================\n",
      "STEP 2: CREATING BASE GRID\n",
      "================================================================================\n",
      "\n",
      "Transforming data from wide to long format...\n",
      " Grid created with 58,327,370 rows\n",
      "\n",
      "Adding test period rows...\n",
      " Added test period: 853,720 rows\n",
      " Total grid size: 59,181,090 rows\n",
      "\n",
      "Optimizing data types...\n",
      " Memory optimized\n",
      "\n",
      "================================================================================\n",
      "STEP 3: ADDING PRICE FEATURES\n",
      "================================================================================\n",
      "\n",
      "Merging calendar week information...\n",
      "Calculating price statistics...\n",
      "Merging prices to grid...\n",
      " Price features added\n",
      " Grid shape: (59181090, 14)\n",
      "\n",
      "================================================================================\n",
      "STEP 4: ADDING CALENDAR FEATURES\n",
      "================================================================================\n",
      "\n",
      "Merging calendar information...\n",
      "Extracting date features...\n",
      " Calendar features added\n",
      " Grid shape: (59181090, 29)\n",
      "\n",
      "================================================================================\n",
      "STEP 5: ADDING LAG FEATURES\n",
      "================================================================================\n",
      "\n",
      "Filtering data from day 1000...\n",
      " Grid size after filtering: 28,721,580 rows\n",
      "\n",
      "Creating lag features:\n",
      "  - lag_7 (sales from 7 days ago)\n",
      "  - lag_28 (sales from 28 days ago)\n",
      " Lag features added\n",
      " Grid shape: (28721580, 31)\n",
      "\n",
      "================================================================================\n",
      "STEP 6: ADDING ROLLING WINDOW FEATURES\n",
      "================================================================================\n",
      "\n",
      "Creating rolling features:\n",
      "\n",
      "  Window size: 7 days\n",
      "    - rolling_mean_7\n",
      "    - rolling_std_7\n",
      "    - rolling_min_7\n",
      "    - rolling_max_7\n",
      "\n",
      "  Window size: 28 days\n",
      "    - rolling_mean_28\n",
      "    - rolling_std_28\n",
      "    - rolling_min_28\n",
      "    - rolling_max_28\n",
      "\n",
      " Rolling features added\n",
      " Grid shape: (28721580, 39)\n",
      "\n",
      "================================================================================\n",
      "STEP 7: ADDING CUSTOM FEATURES\n",
      "================================================================================\n",
      "\n",
      "Creating custom features:\n",
      "  - Holiday indicators (christmas, thanksgiving, new_year)\n",
      "  - Sales velocity\n",
      "  - Price change indicators\n",
      "  - Seasonal indicators (summer, winter)\n",
      "  - Week of month\n",
      "\n",
      " Custom features added\n",
      " Grid shape: (28721580, 48)\n",
      "\n",
      "================================================================================\n",
      "STEP 8: HANDLING MISSING VALUES\n",
      "================================================================================\n",
      "\n",
      "Missing values before: 129,797,890\n",
      "\n",
      "Handling lag features:\n",
      "  - Forward filling 2 lag features\n",
      "\n",
      "Handling rolling features:\n",
      "  - Median filling 8 rolling features\n",
      "\n",
      "Handling price features:\n",
      "  - Filling sell_price\n",
      "  - Filling price_norm\n",
      "  - Filling price_max\n",
      "  - Filling price_min\n",
      "  - Filling price_mean\n",
      "  - Filling price_std\n",
      "\n",
      "Filling remaining NaN...\n",
      "  - Filling 40 numerical columns with 0\n",
      "  - Filling 8 categorical columns\n",
      "\n",
      " Missing values after: 0\n",
      " All missing values handled\n",
      "\n",
      "================================================================================\n",
      "STEP 9: FEATURE SELECTION\n",
      "================================================================================\n",
      "\n",
      " Total features: 40\n",
      "  - Categorical: 4\n",
      "  - Numerical: 36\n",
      "\n",
      "Sample features (first 15):\n",
      "   1. sell_price                     (numerical)\n",
      "   2. price_norm                     (numerical)\n",
      "   3. price_max                      (numerical)\n",
      "   4. price_min                      (numerical)\n",
      "   5. price_mean                     (numerical)\n",
      "   6. price_std                      (numerical)\n",
      "   7. event_name_1                   (categorical)\n",
      "   8. event_type_1                   (categorical)\n",
      "   9. event_name_2                   (categorical)\n",
      "  10. event_type_2                   (categorical)\n",
      "  11. snap_CA                        (numerical)\n",
      "  12. snap_TX                        (numerical)\n",
      "  13. snap_WI                        (numerical)\n",
      "  14. day                            (numerical)\n",
      "  15. week                           (numerical)\n",
      "  ... and 25 more features\n",
      "\n",
      "================================================================================\n",
      "STEP 10: TRAIN/VALIDATION/TEST SPLIT\n",
      "================================================================================\n",
      "\n",
      "Creating time-based splits...\n",
      "\n",
      " Training set: 27,014,140 rows\n",
      "    Date range: d_1000 to d_1885\n",
      "\n",
      " Validation set: 853,720 rows\n",
      "    Date range: d_1886 to d_1913\n",
      "\n",
      " Test set: 853,720 rows\n",
      "    Date range: d_1914 to d_1941\n",
      "\n",
      "================================================================================\n",
      "STEP 11: SAVING PROCESSED DATA\n",
      "================================================================================\n",
      "\n",
      "Saving processed dataset to processed_data.pkl...\n",
      " Dataset saved (28,721,580 rows, 48 columns)\n",
      "\n",
      "Saving feature information...\n",
      " Feature information saved to feature_info.pkl\n",
      "\n",
      "Creating feature summary report...\n",
      " Feature summary saved to feature_summary.txt\n",
      "\n",
      "================================================================================\n",
      "FEATURE ENGINEERING COMPLETED SUCCESSFULLY!\n",
      "================================================================================\n",
      "\n",
      "Generated files:\n",
      "   processed_data.pkl - Complete processed dataset\n",
      "   feature_info.pkl - Feature names and types\n",
      "   feature_summary.txt - Detailed feature report\n",
      "\n",
      "Dataset statistics:\n",
      "  - Total rows: 28,721,580\n",
      "  - Total columns: 48\n",
      "  - Features: 40\n",
      "  - Training samples: 27,014,140\n",
      "  - Validation samples: 853,720\n",
      "  - Test samples: 853,720\n",
      "\n",
      "Next step: Run Part 2 (model_training.py) for model training!\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# EXECUTE\n",
    "if __name__ == '__main__':\n",
    "    run_feature_engineering_pipeline()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 1236839,
     "sourceId": 18599,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 31192,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
