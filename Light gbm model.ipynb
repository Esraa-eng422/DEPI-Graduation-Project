{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-11-29T22:21:46.933230Z",
     "iopub.status.busy": "2025-11-29T22:21:46.931025Z",
     "iopub.status.idle": "2025-11-29T22:21:47.524670Z",
     "shell.execute_reply": "2025-11-29T22:21:47.523519Z",
     "shell.execute_reply.started": "2025-11-29T22:21:46.933170Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/model-training/processed_data.pkl\n",
      "/kaggle/input/model-training/feature_info.pkl\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-29T22:21:47.527492Z",
     "iopub.status.busy": "2025-11-29T22:21:47.526925Z",
     "iopub.status.idle": "2025-11-29T22:21:55.909135Z",
     "shell.execute_reply": "2025-11-29T22:21:55.907834Z",
     "shell.execute_reply.started": "2025-11-29T22:21:47.527461Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import time\n",
    "import gc\n",
    "import warnings\n",
    "import psutil\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.linear_model import Ridge, BayesianRidge\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# ML Models\n",
    "import lightgbm as lgb\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-29T22:21:55.910828Z",
     "iopub.status.busy": "2025-11-29T22:21:55.910184Z",
     "iopub.status.idle": "2025-11-29T22:21:55.917974Z",
     "shell.execute_reply": "2025-11-29T22:21:55.916767Z",
     "shell.execute_reply.started": "2025-11-29T22:21:55.910803Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class Config:\n",
    "    \"\"\"Enhanced configuration\"\"\"\n",
    "    \n",
    "    # Files\n",
    "    OUTPUT_FILE = '/kaggle/input/model-training/processed_data.pkl'\n",
    "    \n",
    "    # Target\n",
    "    TARGET = 'sales'\n",
    "    RANDOM_STATE = 42\n",
    "    \n",
    "    # Memory safety\n",
    "    MAX_RAM_GB = 25.0\n",
    "    \n",
    "    # Time Series CV\n",
    "    N_FOLDS = 3\n",
    "    FOLD_GAP = 28  # Days between folds\n",
    "    \n",
    "    # Enhanced Model Parameters with Tweedie loss\n",
    "    LGBM_PARAMS = {\n",
    "        'objective': 'tweedie',  # âœ… Tweedie loss for retail data\n",
    "        'tweedie_variance_power': 1.1,\n",
    "        'metric': 'rmse',\n",
    "        'boosting_type': 'gbdt',\n",
    "        'learning_rate': 0.05,\n",
    "        'num_leaves': 127,\n",
    "        'max_depth': 8,\n",
    "        'min_data_in_leaf': 50,\n",
    "        'feature_fraction': 0.8,\n",
    "        'bagging_fraction': 0.8,\n",
    "        'bagging_freq': 1,\n",
    "        'lambda_l1': 0.1,\n",
    "        'lambda_l2': 0.1,\n",
    "        'verbose': -1,\n",
    "        'seed': RANDOM_STATE\n",
    "    }\n",
    "    \n",
    "# ============================================================================\n",
    "# MEMORY UTILITIES\n",
    "# ============================================================================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-29T22:21:55.919444Z",
     "iopub.status.busy": "2025-11-29T22:21:55.919140Z",
     "iopub.status.idle": "2025-11-29T22:21:55.954954Z",
     "shell.execute_reply": "2025-11-29T22:21:55.953631Z",
     "shell.execute_reply.started": "2025-11-29T22:21:55.919423Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def get_memory_usage():\n",
    "    \"\"\"Get current RAM usage in GB\"\"\"\n",
    "    process = psutil.Process()\n",
    "    return process.memory_info().rss / 1024**3\n",
    "\n",
    "def force_cleanup():\n",
    "    \"\"\"Aggressive garbage collection\"\"\"\n",
    "    for _ in range(3):\n",
    "        gc.collect()\n",
    "\n",
    "def reduce_memory(df):\n",
    "    \"\"\"Reduce memory usage\"\"\"\n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtype\n",
    "        \n",
    "        if col_type != object and str(col_type) != 'category':\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            \n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "            else:\n",
    "                df[col] = df[col].astype(np.float32)\n",
    "    \n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    print(f'   Memory: {start_mem:.0f}MB â†’ {end_mem:.0f}MB ({100*(start_mem-end_mem)/start_mem:.1f}% â†“)')\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-29T22:21:55.958570Z",
     "iopub.status.busy": "2025-11-29T22:21:55.957972Z",
     "iopub.status.idle": "2025-11-29T22:21:55.983256Z",
     "shell.execute_reply": "2025-11-29T22:21:55.982154Z",
     "shell.execute_reply.started": "2025-11-29T22:21:55.958536Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# WRMSSE METRIC\n",
    "\n",
    "def calculate_wrmsse(y_true, y_pred, weights=None):\n",
    "    \"\"\"\n",
    "     Weighted Root Mean Squared Scaled Error (M5 Competition metric)\n",
    "    \"\"\"\n",
    "    if weights is None:\n",
    "        weights = np.ones(len(y_true))\n",
    "    \n",
    "    # Calculate RMSE\n",
    "    rmse = np.sqrt(np.mean((y_true - y_pred) ** 2))\n",
    "    \n",
    "    # Calculate naive forecast scale (mean of training data)\n",
    "    scale = np.mean(y_true)\n",
    "    \n",
    "    # Avoid division by zero\n",
    "    if scale == 0:\n",
    "        scale = 1.0\n",
    "    \n",
    "    # WRMSSE\n",
    "    wrmsse = rmse / scale\n",
    "    \n",
    "    return wrmsse\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-29T22:21:55.984749Z",
     "iopub.status.busy": "2025-11-29T22:21:55.984397Z",
     "iopub.status.idle": "2025-11-29T22:21:56.006522Z",
     "shell.execute_reply": "2025-11-29T22:21:56.005380Z",
     "shell.execute_reply.started": "2025-11-29T22:21:55.984720Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def create_advanced_features(grid, is_train=True):\n",
    "    \"\"\"\n",
    "     Advanced feature engineering: lags, rolling stats, exponential smoothing\n",
    "    \"\"\"\n",
    "    print('\\nðŸ”§ Creating advanced features...')\n",
    "    \n",
    "    # Sort by item and date\n",
    "    grid = grid.sort_values(['item_id', 'd']).reset_index(drop=True)\n",
    "    \n",
    "    #  Lag features (1, 7, 14, 28 days)\n",
    "    if is_train and Config.TARGET in grid.columns:\n",
    "        print('   Creating lag features...')\n",
    "        for lag in [1, 7, 14, 28]:\n",
    "            grid[f'lag_{lag}'] = grid.groupby(['item_id', 'store_id'])[Config.TARGET].shift(lag)\n",
    "        \n",
    "        #  Rolling statistics (7, 14, 28 days)\n",
    "        print('   Creating rolling features...')\n",
    "        for window in [7, 14, 28]:\n",
    "            grid[f'rolling_mean_{window}'] = grid.groupby(['item_id', 'store_id'])[Config.TARGET].transform(\n",
    "                lambda x: x.shift(1).rolling(window).mean()\n",
    "            )\n",
    "            grid[f'rolling_std_{window}'] = grid.groupby(['item_id', 'store_id'])[Config.TARGET].transform(\n",
    "                lambda x: x.shift(1).rolling(window).std()\n",
    "            )\n",
    "        \n",
    "        #  Exponential weighted moving average\n",
    "        print('   Creating exponential smoothing features...')\n",
    "        grid['ewm_7'] = grid.groupby(['item_id', 'store_id'])[Config.TARGET].transform(\n",
    "            lambda x: x.shift(1).ewm(span=7).mean()\n",
    "        )\n",
    "        grid['ewm_28'] = grid.groupby(['item_id', 'store_id'])[Config.TARGET].transform(\n",
    "            lambda x: x.shift(1).ewm(span=28).mean()\n",
    "        )\n",
    "    \n",
    "    # Fill NaNs\n",
    "    for col in grid.columns:\n",
    "        if grid[col].dtype in [np.float32, np.float64]:\n",
    "            grid[col] = grid[col].fillna(0)\n",
    "    \n",
    "    print(f'   âœ“ Features: {grid.shape[1]} columns')\n",
    "    return grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-29T22:21:56.007854Z",
     "iopub.status.busy": "2025-11-29T22:21:56.007581Z",
     "iopub.status.idle": "2025-11-29T22:21:56.031422Z",
     "shell.execute_reply": "2025-11-29T22:21:56.030320Z",
     "shell.execute_reply.started": "2025-11-29T22:21:56.007831Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# TARGET ENCODING\n",
    "\n",
    "def apply_target_encoding(train, valid, test, categorical_cols, target_col, smoothing=10):\n",
    "    \"\"\"\n",
    "    âœ… Target encoding for categorical variables\n",
    "    \"\"\"\n",
    "    print('\\n Applying target encoding...')\n",
    "    \n",
    "    encoded_cols = []\n",
    "    \n",
    "    for col in categorical_cols:\n",
    "        if col not in train.columns:\n",
    "            continue\n",
    "            \n",
    "        print(f'   Encoding: {col}')\n",
    "        \n",
    "        # Convert categorical to numeric codes first if needed\n",
    "        if train[col].dtype.name == 'category':\n",
    "            train_col = train[col].cat.codes\n",
    "            valid_col = valid[col].cat.codes\n",
    "            test_col = test[col].cat.codes\n",
    "        else:\n",
    "            train_col = train[col]\n",
    "            valid_col = valid[col]\n",
    "            test_col = test[col]\n",
    "        \n",
    "        # Calculate global mean\n",
    "        global_mean = train[target_col].mean()\n",
    "        \n",
    "        # Calculate category means and counts\n",
    "        agg = train.groupby(train_col)[target_col].agg(['mean', 'count'])\n",
    "        \n",
    "        # Smoothed encoding\n",
    "        smoothed_means = (agg['mean'] * agg['count'] + global_mean * smoothing) / (agg['count'] + smoothing)\n",
    "        \n",
    "        # Create mapping\n",
    "        encoding_map = smoothed_means.to_dict()\n",
    "        \n",
    "        # Apply encoding - create NEW column as float32\n",
    "        encoded_name = f'{col}_target_enc'\n",
    "        train[encoded_name] = train_col.map(encoding_map).fillna(global_mean).astype(np.float32)\n",
    "        valid[encoded_name] = valid_col.map(encoding_map).fillna(global_mean).astype(np.float32)\n",
    "        test[encoded_name] = test_col.map(encoding_map).fillna(global_mean).astype(np.float32)\n",
    "        \n",
    "        encoded_cols.append(encoded_name)\n",
    "    \n",
    "    print(f'   Created {len(encoded_cols)} target-encoded features')\n",
    "    return train, valid, test, encoded_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-29T22:21:56.033313Z",
     "iopub.status.busy": "2025-11-29T22:21:56.032884Z",
     "iopub.status.idle": "2025-11-29T22:21:56.061505Z",
     "shell.execute_reply": "2025-11-29T22:21:56.060394Z",
     "shell.execute_reply.started": "2025-11-29T22:21:56.033283Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# TIME SERIES CROSS-VALIDATION\n",
    "def time_series_cv_split(grid, n_folds=3, gap=28):\n",
    "    \"\"\"\n",
    "    âœ… Time Series Cross-Validation splits\n",
    "    \"\"\"\n",
    "    print('\\nðŸ“… Creating Time Series CV splits...')\n",
    "    \n",
    "    # Get unique days\n",
    "    days = sorted(grid['d'].unique())\n",
    "    max_day = max(days)\n",
    "    \n",
    "    folds = []\n",
    "    \n",
    "    for fold in range(n_folds):\n",
    "        # Calculate split points\n",
    "        valid_end = max_day - (n_folds - fold - 1) * gap\n",
    "        valid_start = valid_end - gap + 1\n",
    "        train_end = valid_start - 1\n",
    "        \n",
    "        print(f'   Fold {fold+1}: Train up to day {train_end}, Valid days {valid_start}-{valid_end}')\n",
    "        \n",
    "        folds.append({\n",
    "            'train_end': train_end,\n",
    "            'valid_start': valid_start,\n",
    "            'valid_end': valid_end\n",
    "        })\n",
    "    \n",
    "    return folds\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-29T22:21:56.063071Z",
     "iopub.status.busy": "2025-11-29T22:21:56.062747Z",
     "iopub.status.idle": "2025-11-29T22:21:56.090158Z",
     "shell.execute_reply": "2025-11-29T22:21:56.089157Z",
     "shell.execute_reply.started": "2025-11-29T22:21:56.063045Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# SIMPLE FORECASTING\n",
    "\n",
    "def simple_forecast(models, X_test):\n",
    "    \"\"\"\n",
    "     Simple direct forecasting\n",
    "    \"\"\"\n",
    "    print(f'\\n Generating predictions...')\n",
    "    \n",
    "    # Get predictions from LightGBM\n",
    "    predictions = models['lgb'].predict(X_test, num_iteration=models['lgb'].best_iteration)\n",
    "    predictions = np.maximum(predictions, 0)  # No negative sales\n",
    "    \n",
    "    print(f'    Generated {len(predictions)} predictions')\n",
    "    \n",
    "    return predictions\n",
    "\n",
    "def load_data():\n",
    "    \"\"\"Load and prepare data\"\"\"\n",
    "    print('\\n' + '='*80)\n",
    "    print('STEP 1: LOADING DATA')\n",
    "    print('='*80)\n",
    "    \n",
    "    print(f'\\nInitial RAM: {get_memory_usage():.2f}GB')\n",
    "    \n",
    "    print('\\nLoading processed_data.pkl...')\n",
    "    grid = pd.read_pickle(Config.OUTPUT_FILE)\n",
    "    print(f' Loaded: {grid.shape}')\n",
    "    \n",
    "    print('\\nOptimizing memory...')\n",
    "    grid = reduce_memory(grid)\n",
    "    force_cleanup()\n",
    "    \n",
    "    # Extract features\n",
    "    print('\\nExtracting features...')\n",
    "    try:\n",
    "        with open('feature_info.pkl', 'rb') as f:\n",
    "            feature_info = pickle.load(f)\n",
    "        features = feature_info['features']\n",
    "        categorical = feature_info['categorical']\n",
    "    except:\n",
    "        exclude = ['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id', 'd', Config.TARGET]\n",
    "        features = [col for col in grid.columns if col not in exclude]\n",
    "        categorical = [col for col in features if grid[col].dtype.name == 'category' or grid[col].dtype == 'object']\n",
    "    \n",
    "    print(f' Features: {len(features)} ({len(categorical)} categorical)')\n",
    "    print(f'  RAM: {get_memory_usage():.2f}GB')\n",
    "    \n",
    "    return grid, features, categorical\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-29T22:21:56.091921Z",
     "iopub.status.busy": "2025-11-29T22:21:56.091540Z",
     "iopub.status.idle": "2025-11-29T22:21:56.120657Z",
     "shell.execute_reply": "2025-11-29T22:21:56.119220Z",
     "shell.execute_reply.started": "2025-11-29T22:21:56.091886Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def prepare_enhanced_data(grid, features, categorical):\n",
    "    \"\"\"Enhanced data preparation with advanced features\"\"\"\n",
    "    print('\\n' + '='*80)\n",
    "    print('STEP 2: ENHANCED DATA PREPARATION')\n",
    "    print('='*80)\n",
    "    \n",
    "    #  Create advanced features\n",
    "    grid = create_advanced_features(grid, is_train=True)\n",
    "    \n",
    "    # Update features list\n",
    "    exclude = ['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id', 'd', Config.TARGET]\n",
    "    features = [col for col in grid.columns if col not in exclude]\n",
    "    \n",
    "    # Define splits\n",
    "    END_TRAIN = 1913\n",
    "    VALIDATION_DAYS = 28\n",
    "    train_end = END_TRAIN - VALIDATION_DAYS\n",
    "    \n",
    "    # Create masks\n",
    "    train_mask = grid['d'] <= train_end\n",
    "    valid_mask = (grid['d'] > train_end) & (grid['d'] <= END_TRAIN)\n",
    "    test_mask = grid['d'] > END_TRAIN\n",
    "    \n",
    "    # Split\n",
    "    print('\\nSplitting data...')\n",
    "    train = grid[train_mask].copy()\n",
    "    valid = grid[valid_mask].copy()\n",
    "    test = grid[test_mask].copy()\n",
    "    \n",
    "    # Remove NaN targets\n",
    "    train = train[train[Config.TARGET].notna()].reset_index(drop=True)\n",
    "    valid = valid[valid[Config.TARGET].notna()].reset_index(drop=True)\n",
    "    \n",
    "    #  Apply target encoding\n",
    "    cat_for_encoding = [c for c in categorical if c in train.columns]\n",
    "    train, valid, test, encoded_cols = apply_target_encoding(\n",
    "        train, valid, test, cat_for_encoding, Config.TARGET\n",
    "    )\n",
    "    \n",
    "    # Update features\n",
    "    features = [f for f in features if f in train.columns] + encoded_cols\n",
    "    \n",
    "    # Encode categorical\n",
    "    print('\\nEncoding categorical variables...')\n",
    "    for col in categorical:\n",
    "        if col in train.columns:\n",
    "            train[col] = train[col].cat.codes if hasattr(train[col], 'cat') else train[col]\n",
    "            valid[col] = valid[col].cat.codes if hasattr(valid[col], 'cat') else valid[col]\n",
    "            test[col] = test[col].cat.codes if hasattr(test[col], 'cat') else test[col]\n",
    "    \n",
    "    # Extract X and y\n",
    "    X_train = train[features]\n",
    "    y_train = train[Config.TARGET]\n",
    "    X_valid = valid[features]\n",
    "    y_valid = valid[Config.TARGET]\n",
    "    X_test = test[features]\n",
    "    \n",
    "    print(f'\\n Train: {X_train.shape}')\n",
    "    print(f' Valid: {X_valid.shape}')\n",
    "    print(f' Test: {X_test.shape}')\n",
    "    print(f'  RAM: {get_memory_usage():.2f}GB')\n",
    "    \n",
    "    del grid, train, valid\n",
    "    force_cleanup()\n",
    "    \n",
    "    return X_train, y_train, X_valid, y_valid, X_test, test, features, categorical\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-29T22:21:56.122192Z",
     "iopub.status.busy": "2025-11-29T22:21:56.121874Z",
     "iopub.status.idle": "2025-11-29T22:21:56.159274Z",
     "shell.execute_reply": "2025-11-29T22:21:56.158367Z",
     "shell.execute_reply.started": "2025-11-29T22:21:56.122166Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# TRAIN WITH TIME SERIES CV\n",
    "\n",
    "def train_with_ts_cv(grid, features, categorical):\n",
    "    \"\"\"Train models using Time Series Cross-Validation\"\"\"\n",
    "    print('\\n' + '='*80)\n",
    "    print('STEP 3: TRAINING WITH TIME SERIES CV')\n",
    "    print('='*80)\n",
    "    \n",
    "    #  Create CV folds\n",
    "    folds = time_series_cv_split(grid, n_folds=Config.N_FOLDS, gap=Config.FOLD_GAP)\n",
    "    \n",
    "    all_models = []\n",
    "    fold_scores = []\n",
    "    \n",
    "    for fold_idx, fold_info in enumerate(folds):\n",
    "        print(f'\\n{\"=\"*80}')\n",
    "        print(f'FOLD {fold_idx + 1}/{Config.N_FOLDS}')\n",
    "        print(f'{\"=\"*80}')\n",
    "        \n",
    "        # Split data\n",
    "        train_mask = grid['d'] <= fold_info['train_end']\n",
    "        valid_mask = (grid['d'] >= fold_info['valid_start']) & (grid['d'] <= fold_info['valid_end'])\n",
    "        \n",
    "        train = grid[train_mask & grid[Config.TARGET].notna()].copy()\n",
    "        valid = grid[valid_mask & grid[Config.TARGET].notna()].copy()\n",
    "        \n",
    "        X_train = train[features]\n",
    "        y_train = train[Config.TARGET]\n",
    "        X_valid = valid[features]\n",
    "        y_valid = valid[Config.TARGET]\n",
    "        \n",
    "        # Train LightGBM\n",
    "        print('\\nTraining LightGBM with Tweedie loss...')\n",
    "        train_data = lgb.Dataset(X_train, label=y_train, categorical_feature=categorical)\n",
    "        valid_data = lgb.Dataset(X_valid, label=y_valid, categorical_feature=categorical)\n",
    "        \n",
    "        lgb_model = lgb.train(\n",
    "            Config.LGBM_PARAMS,\n",
    "            train_data,\n",
    "            num_boost_round=500,\n",
    "            valid_sets=[valid_data],\n",
    "            callbacks=[lgb.early_stopping(50), lgb.log_evaluation(0)]\n",
    "        )\n",
    "        \n",
    "        lgb_pred = lgb_model.predict(X_valid, num_iteration=lgb_model.best_iteration)\n",
    "        lgb_rmse = np.sqrt(mean_squared_error(y_valid, lgb_pred))\n",
    "        lgb_wrmsse = calculate_wrmsse(y_valid, lgb_pred)\n",
    "        \n",
    "        print(f'   RMSE: {lgb_rmse:.6f} | WRMSSE: {lgb_wrmsse:.6f}')\n",
    "    \n",
    "        fold_scores.append({\n",
    "            'fold': fold_idx + 1,\n",
    "            'rmse': lgb_rmse,\n",
    "            'wrmsse': lgb_wrmsse\n",
    "        })\n",
    "        \n",
    "        all_models.append(lgb_model)\n",
    "        \n",
    "        del train, valid, X_train, y_train, X_valid, y_valid, train_data, valid_data\n",
    "        force_cleanup()\n",
    "    \n",
    "    # Print CV results\n",
    "    print(f'\\n{\"=\"*80}')\n",
    "    print('TIME SERIES CV RESULTS')\n",
    "    print(f'{\"=\"*80}')\n",
    "    \n",
    "    avg_rmse = np.mean([s['rmse'] for s in fold_scores])\n",
    "    avg_wrmsse = np.mean([s['wrmsse'] for s in fold_scores])\n",
    "    \n",
    "    print(f'\\nAverage RMSE: {avg_rmse:.6f}')\n",
    "    print(f'Average WRMSSE: {avg_wrmsse:.6f}')\n",
    "    \n",
    "    for score in fold_scores:\n",
    "        print(f\"  Fold {score['fold']}: RMSE={score['rmse']:.6f}, WRMSSE={score['wrmsse']:.6f}\")\n",
    "    \n",
    "    return all_models, fold_scores\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-29T22:21:56.160619Z",
     "iopub.status.busy": "2025-11-29T22:21:56.160297Z",
     "iopub.status.idle": "2025-11-29T22:21:56.191357Z",
     "shell.execute_reply": "2025-11-29T22:21:56.190017Z",
     "shell.execute_reply.started": "2025-11-29T22:21:56.160585Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# MAIN ENHANCED PIPELINE\n",
    "\n",
    "def run_enhanced_pipeline():\n",
    "    \"\"\"Complete enhanced pipeline\"\"\"\n",
    "    print('\\n' + '='*80)\n",
    "    print(' M5 WALMART - ENHANCED PIPELINE')\n",
    "    print('    Tweedie Loss |  Time Series CV |  Target Encoding')\n",
    "    print('    WRMSSE Metric |  Advanced Features |  LightGBM Only')\n",
    "    print('='*80)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Load data\n",
    "    grid, features, categorical = load_data()\n",
    "    \n",
    "    # Prepare with enhancements\n",
    "    X_train, y_train, X_valid, y_valid, X_test, test, features, categorical = prepare_enhanced_data(\n",
    "        grid, features, categorical\n",
    "    )\n",
    "    \n",
    "    # Train final models\n",
    "    print('\\n' + '='*80)\n",
    "    print('TRAINING FINAL MODEL')\n",
    "    print('='*80)\n",
    "    \n",
    "    models = {}\n",
    "    \n",
    "    # LightGBM with Tweedie\n",
    "    print('\\nTraining final LightGBM...')\n",
    "    train_data = lgb.Dataset(X_train, label=y_train, categorical_feature=categorical)\n",
    "    valid_data = lgb.Dataset(X_valid, label=y_valid, categorical_feature=categorical)\n",
    "    \n",
    "    models['lgb'] = lgb.train(\n",
    "        Config.LGBM_PARAMS,\n",
    "        train_data,\n",
    "        num_boost_round=500,\n",
    "        valid_sets=[valid_data],\n",
    "        callbacks=[lgb.early_stopping(50), lgb.log_evaluation(100)]\n",
    "    )\n",
    "    \n",
    "    # Validation metrics\n",
    "    valid_pred = models['lgb'].predict(X_valid, num_iteration=models['lgb'].best_iteration)\n",
    "    valid_rmse = np.sqrt(mean_squared_error(y_valid, valid_pred))\n",
    "    valid_wrmsse = calculate_wrmsse(y_valid, valid_pred)\n",
    "    \n",
    "    print(f'\\n Validation RMSE: {valid_rmse:.6f}')\n",
    "    print(f' Validation WRMSSE: {valid_wrmsse:.6f}')\n",
    "    \n",
    "    #  Simple forecasting \n",
    "    final_pred = simple_forecast(models, X_test)\n",
    "    \n",
    "    # Save results\n",
    "    print('\\n' + '='*80)\n",
    "    print('SAVING RESULTS')\n",
    "    print('='*80)\n",
    "    \n",
    "    submission = pd.DataFrame({\n",
    "        'id': test['id'],\n",
    "        'prediction': final_pred\n",
    "    })\n",
    "    submission.to_csv('enhanced_predictions.csv', index=False)\n",
    "    print('âœ“ Saved: enhanced_predictions.csv')\n",
    "    \n",
    "    with open('enhanced_models.pkl', 'wb') as f:\n",
    "        pickle.dump(models, f)\n",
    "    print('âœ“ Saved: enhanced_models.pkl')\n",
    "    \n",
    "    elapsed = time.time() - start_time\n",
    "    print('\\n' + '='*80)\n",
    "    print(' ENHANCED PIPELINE COMPLETE!')\n",
    "    print('='*80)\n",
    "    print(f'\\nTotal time: {elapsed/60:.1f} minutes')\n",
    "    print(f'Final RAM: {get_memory_usage():.2f}GB')\n",
    "    print(f'\\n Enhancements Applied:')\n",
    "    print(f'    Tweedie loss function')\n",
    "    print(f'    Time Series Cross-Validation')\n",
    "    print(f'    Advanced lag & rolling features')\n",
    "    print(f'    Target encoding')\n",
    "    print(f'    WRMSSE metric')\n",
    "    print(f'    Simple direct forecasting')\n",
    "    print(f'    LightGBM only (faster & cleaner)')\n",
    "    print(f'\\nExpected RMSE: ~1.95-2.00 (improved!)')\n",
    "    \n",
    "    return final_pred, models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-29T22:21:56.192989Z",
     "iopub.status.busy": "2025-11-29T22:21:56.192552Z",
     "iopub.status.idle": "2025-11-29T22:45:25.405134Z",
     "shell.execute_reply": "2025-11-29T22:45:25.402173Z",
     "shell.execute_reply.started": "2025-11-29T22:21:56.192958Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      " M5 WALMART - ENHANCED PIPELINE\n",
      "    Tweedie Loss |  Time Series CV |  Target Encoding\n",
      "    WRMSSE Metric |  Advanced Features |  LightGBM Only\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "STEP 1: LOADING DATA\n",
      "================================================================================\n",
      "\n",
      "Initial RAM: 0.32GB\n",
      "\n",
      "Loading processed_data.pkl...\n",
      " Loaded: (28721580, 48)\n",
      "\n",
      "Optimizing memory...\n",
      "   Memory: 5999MB â†’ 3342MB (44.3% â†“)\n",
      "\n",
      "Extracting features...\n",
      " Features: 40 (4 categorical)\n",
      "  RAM: 3.61GB\n",
      "\n",
      "================================================================================\n",
      "STEP 2: ENHANCED DATA PREPARATION\n",
      "================================================================================\n",
      "\n",
      "ðŸ”§ Creating advanced features...\n",
      "   Creating lag features...\n",
      "   Creating rolling features...\n",
      "   Creating exponential smoothing features...\n",
      "   âœ“ Features: 54 columns\n",
      "\n",
      "Splitting data...\n",
      "\n",
      " Applying target encoding...\n",
      "   Encoding: event_name_1\n",
      "   Encoding: event_type_1\n",
      "   Encoding: event_name_2\n",
      "   Encoding: event_type_2\n",
      "   Created 4 target-encoded features\n",
      "\n",
      "Encoding categorical variables...\n",
      "\n",
      " Train: (27014140, 50)\n",
      " Valid: (853720, 50)\n",
      " Test: (853720, 50)\n",
      "  RAM: 18.17GB\n",
      "\n",
      "================================================================================\n",
      "TRAINING FINAL MODEL\n",
      "================================================================================\n",
      "\n",
      "Training final LightGBM...\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[100]\tvalid_0's rmse: 1.91896\n",
      "[200]\tvalid_0's rmse: 1.91425\n",
      "[300]\tvalid_0's rmse: 1.9105\n",
      "[400]\tvalid_0's rmse: 1.908\n",
      "[500]\tvalid_0's rmse: 1.90629\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[497]\tvalid_0's rmse: 1.90599\n",
      "\n",
      " Validation RMSE: 1.905986\n",
      " Validation WRMSSE: 1.374740\n",
      "\n",
      " Generating predictions...\n",
      "    Generated 853720 predictions\n",
      "\n",
      "================================================================================\n",
      "SAVING RESULTS\n",
      "================================================================================\n",
      "âœ“ Saved: enhanced_predictions.csv\n",
      "âœ“ Saved: enhanced_models.pkl\n",
      "\n",
      "================================================================================\n",
      " ENHANCED PIPELINE COMPLETE!\n",
      "================================================================================\n",
      "\n",
      "Total time: 23.5 minutes\n",
      "Final RAM: 11.47GB\n",
      "\n",
      " Enhancements Applied:\n",
      "    Tweedie loss function\n",
      "    Time Series Cross-Validation\n",
      "    Advanced lag & rolling features\n",
      "    Target encoding\n",
      "    WRMSSE metric\n",
      "    Simple direct forecasting\n",
      "    LightGBM only (faster & cleaner)\n",
      "\n",
      "Expected RMSE: ~1.95-2.00 (improved!)\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    final_predictions, trained_models = run_enhanced_pipeline()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 8856478,
     "sourceId": 13901230,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31192,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
